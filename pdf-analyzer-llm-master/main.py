import os
import streamlit as st
import tempfile
import re
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain_core.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms.ollama import Ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
import language_tool_python

# Ortam deÄŸiÅŸkeni
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"

# YazÄ±m denetimi baÅŸlat
try:
    _turk_tool = language_tool_python.LanguageTool("tr")
except Exception:
    _turk_tool = None

# Karakter dÃ¼zeltici
def fix_encoding(text: str) -> str:
    text = text.encode("utf-8", "ignore").decode("utf-8", "ignore")
    text = text.replace("Ã½", "Ä±").replace("Ã¾", "ÅŸ").replace("Ã°", "ÄŸ") \
               .replace("Ã", "Ä°").replace("Ã", "Å").replace("Ã", "Ä")
    return text

# YanÄ±t temizleyici
def clean_output(text: str) -> str:
    text = re.sub(r"<think[^>]*>.*?</think[^>]*>", "", text, flags=re.I | re.S)
    text = re.sub(r"<.*?>", "", text, flags=re.I | re.S)
    text = re.sub(r"\s{2,}", " ", text)
    return fix_encoding(text.strip())

# BaÅŸlÄ±k
st.title("ğŸ“š TÃ¼rkÃ§e PDF RAG Chatbot + Qwen3:8B")
st.markdown("Qwen3:8B â€¢ Ollama â€¢ LangChain â€¢ FAISS â€¢ Ã‡Ä±karÄ±m + Karakter OnarÄ±mÄ± + Tekrar Engelleme")

db_path = "faiss_index"
uploaded = st.file_uploader("PDF yÃ¼kle (.pdf)", type="pdf")

# Oturum geÃ§miÅŸi
if "history" not in st.session_state:
    st.session_state.history = []

# PDF yÃ¼klendiÄŸinde Ã§alÄ±ÅŸ
if uploaded and "retriever" not in st.session_state:
    with st.spinner("PDF iÅŸleniyor..."):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded.read())
            pdf_path = tmp.name

        loader = PyPDFLoader(pdf_path)
        raw_docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=150)
        docs = text_splitter.split_documents(raw_docs)

        # PDF iÃ§eriÄŸini Ã¶nceden karakter dÃ¼zelt
        for doc in docs:
            doc.page_content = fix_encoding(doc.page_content)

        embed = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-small",
            model_kwargs={"device": "cpu"}
        )

        vectordb = FAISS.from_documents(docs, embed)
        vectordb.save_local(db_path)
        #retriever = vectordb.as_retriever(search_kwargs={"k": 5})
        retriever = vectordb.as_retriever(search_kwargs={"k": 8})

        st.session_state.retriever = retriever
        st.session_state.docs = docs

        os.remove(pdf_path)

# Model ve zincir
if "retriever" in st.session_state:
    llm = Ollama(
        model="qwen3:8b",
        base_url="http://host.docker.internal:11434",
        temperature=0.3
    )

    answer_prompt = PromptTemplate.from_template("""
AÅŸaÄŸÄ±daki belgeye dayanarak soruyu yanÄ±tla.

YanÄ±t verirken:
- Ã–ncelikle belgeyi dikkatlice analiz et.
- Belgedeki doÄŸrudan bilgileri aÃ§Ä±kÃ§a belirt.
- EÄŸer doÄŸrudan bilgi yoksa, sadece belgeye dayalÄ± mantÄ±klÄ± Ã§Ä±karÄ±mlar yap.
- Tahmin veya dÄ±ÅŸ bilgi kullanma.
- AynÄ± baÅŸlÄ±ÄŸÄ± veya maddeyi tekrar etme.
- Ã–zetlemen isteniyorsa baÅŸlÄ±klarÄ± sadece tekrarlama, iÃ§erikleri anlamlÄ± ÅŸekilde sadeleÅŸtir.
- Teknik terimleri ve sayÄ±sal deÄŸerleri aynen aktar.
- TÃ¼rkÃ§e karakterleri eksiksiz koru.
- <think> gibi dÃ¼ÅŸÃ¼nce yazÄ±larÄ± kullanma.

### Belge:
{context}

### Soru:
{question}

### YanÄ±t:
""")

    condense_prompt = PromptTemplate.from_template("Sadece soruyu sadeleÅŸtir: {question}")
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, k=5)

    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=st.session_state.retriever,
        memory=memory,
        condense_question_prompt=condense_prompt,
        combine_docs_chain_kwargs={"prompt": answer_prompt}
    )

    q = st.text_input("Sorunuz:")
    if q:
        with st.spinner("Cevap hazÄ±rlanÄ±yor..."):
            try:
                result = qa.invoke({"question": q})
                raw_answer = result["answer"]
            except Exception as e:
                st.error(f"Model Ã§aÄŸrÄ±sÄ±nda hata oluÅŸtu: {e}")
                raw_answer = "**YETERSÄ°Z VERÄ°**"

        response = clean_output(raw_answer)

        if _turk_tool and 10 < len(response) < 200:
            try:
                response = _turk_tool.correct(response)
            except Exception:
                pass

        st.session_state.history += [("Siz", q), ("Asistan", response)]

    for role, msg in st.session_state.history:
        st.markdown(f"**{role}:** {msg}")

else:
    st.info("LÃ¼tfen bir PDF yÃ¼kleyin.")

st.sidebar.markdown("**Model:** qwen3:8b - Ollama GGUF")
#docs = st.session_state.retriever.get_relevant_documents(q)
#context = "\n".join([doc.page_content for doc in docs])
#st.markdown("### ğŸ” Modelin GÃ¶rdÃ¼ÄŸÃ¼ BaÄŸlam:")
#st.write(context)

